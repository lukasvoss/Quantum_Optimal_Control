{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[BraketBackend[Aria 1],\n",
       " BraketBackend[Aria 2],\n",
       " BraketBackend[Aspen-M-3],\n",
       " BraketBackend[Forte 1],\n",
       " BraketBackend[Harmony],\n",
       " BraketBackend[Lucy],\n",
       " BraketBackend[SV1],\n",
       " BraketBackend[dm1]]"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from qiskit_braket_provider import AWSBraketProvider\n",
    "from braket.aws import AwsSession\n",
    "\n",
    "aws_session = AwsSession(default_bucket=\"amazon-braket-us-west-1-lukasvoss\")\n",
    "AWSBraketProvider().backends()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/lukasvoss/anaconda3/envs/braket/lib/python3.10/site-packages/qiskit_dynamics/dispatch/backends/jax.py:34: UserWarning: The functionality in the perturbation module of Qiskit Dynamics requires a JAX version <= 0.4.6, due to a bug in JAX versions > 0.4.6. For versions 0.4.4, 0.4.5, and 0.4.6, using the perturbation module functionality requires setting os.environ['JAX_JIT_PJIT_API_MERGE'] = '0' before importing JAX or Dynamics.\n",
      "  warnings.warn(\n",
      "WARNING: All log messages before absl::InitializeLog() is called are written to STDERR\n",
      "I0000 00:00:1707965043.525374       1 tfrt_cpu_pjrt_client.cc:349] TfrtCpuClient created.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Circuit context\n",
      "global phase: π\n",
      "     ┌──────────┐┌────────┐                    ┌──────────┐┌────────┐\n",
      "q_0: ┤ Ry(-π/2) ├┤ Rz(-π) ├──■────■────■────■──┤ Ry(-π/2) ├┤ Rz(-π) ├\n",
      "     └──────────┘└────────┘┌─┴─┐  │    │    │  └──────────┘└────────┘\n",
      "q_1: ──────────────────────┤ X ├──┼────┼────┼────────────────────────\n",
      "                           └───┘┌─┴─┐  │    │                        \n",
      "q_2: ───────────────────────────┤ X ├──┼────┼────────────────────────\n",
      "                                └───┘┌─┴─┐  │                        \n",
      "q_3: ────────────────────────────────┤ X ├──┼────────────────────────\n",
      "                                     └───┘┌─┴─┐                      \n",
      "q_4: ─────────────────────────────────────┤ X ├──────────────────────\n",
      "                                          └───┘                      \n",
      "Starting Rabi experiment for qubit 0...\n",
      "Rabi experiment for qubit 0 done.\n",
      "Starting Drag experiment for qubit 0...\n",
      "Drag experiments done for qubit 0 done.\n",
      "Starting Rabi experiment for qubit 1...\n",
      "Rabi experiment for qubit 1 done.\n",
      "Starting Drag experiment for qubit 1...\n",
      "Drag experiments done for qubit 1 done.\n",
      "All single qubit calibrations are done\n",
      "Updated Instruction Schedule Map <InstructionScheduleMap(1Q instructions:\n",
      "  q0: {'delay', 'id', 'reset', 'sdg', 'rz', 'tdg', 'sx', 'z', 't', 'measure', 'x', 'h', 's'}\n",
      "  q1: {'delay', 'id', 'reset', 'sdg', 'rz', 'tdg', 'sx', 'z', 't', 'measure', 'x', 'h', 's'}\n",
      "Multi qubit instructions:\n",
      "  (0, 1): {'ecr', 'cr45m', 'cr45p'}\n",
      "  (1, 0): {'ecr', 'cr45m', 'cr45p'}\n",
      ")>\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/lukasvoss/anaconda3/envs/braket/lib/python3.10/site-packages/qiskit_experiments/calibration_management/calibrations.py:1391: UserWarning: Schedules are only saved in text format. They cannot be re-loaded.\n",
      "  warnings.warn(\"Schedules are only saved in text format. They cannot be re-loaded.\")\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import sys\n",
    "import os\n",
    "os.environ[\"KMP_DUPLICATE_LIB_OK\"] = \"True\"\n",
    "module_path = os.path.abspath(os.path.join('/Users/lukasvoss/Documents/Master Wirtschaftsphysik/Masterarbeit Yale-NUS CQT/Quantum_Optimal_Control'))\n",
    "if module_path not in sys.path:\n",
    "    sys.path.append(module_path)\n",
    "\n",
    "from template_configurations import gate_q_env_config\n",
    "from quantumenvironment import QuantumEnvironment\n",
    "from gymnasium.wrappers import RescaleAction, ClipAction\n",
    "\n",
    "import logging\n",
    "logging.basicConfig(\n",
    "    level=logging.WARNING,\n",
    "    format=\"%(asctime)s INFO %(message)s\", # hardcoded INFO level\n",
    "    datefmt=\"%Y-%m-%d %H:%M:%S\",\n",
    "    stream=sys.stdout,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Initial lower bounds: [-3.14 -3.14 -3.14 -3.14 -3.14 -3.14 -3.14]\n",
      "Initial upper bounds: [3.14 3.14 3.14 3.14 3.14 3.14 3.14]\n",
      "None\n"
     ]
    },
    {
     "ename": "TypeError",
     "evalue": "unsupported operand type(s) for *: 'NoneType' and 'int'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[3], line 5\u001b[0m\n\u001b[1;32m      2\u001b[0m \u001b[39mprint\u001b[39m(\u001b[39m'\u001b[39m\u001b[39mInitial lower bounds:\u001b[39m\u001b[39m'\u001b[39m, gate_q_env_config\u001b[39m.\u001b[39maction_space\u001b[39m.\u001b[39mlow)\n\u001b[1;32m      3\u001b[0m \u001b[39mprint\u001b[39m(\u001b[39m'\u001b[39m\u001b[39mInitial upper bounds:\u001b[39m\u001b[39m'\u001b[39m, gate_q_env_config\u001b[39m.\u001b[39maction_space\u001b[39m.\u001b[39mhigh)\n\u001b[0;32m----> 5\u001b[0m q_env \u001b[39m=\u001b[39m QuantumEnvironment(gate_q_env_config)\n\u001b[1;32m      7\u001b[0m \u001b[39m# Apply the RescaleAction wrapper\u001b[39;00m\n\u001b[1;32m      8\u001b[0m q_env \u001b[39m=\u001b[39m ClipAction(q_env)\n",
      "File \u001b[0;32m~/Documents/Master Wirtschaftsphysik/Masterarbeit Yale-NUS CQT/Quantum_Optimal_Control/quantumenvironment.py:369\u001b[0m, in \u001b[0;36mQuantumEnvironment.__init__\u001b[0;34m(self, training_config)\u001b[0m\n\u001b[1;32m    364\u001b[0m \u001b[39mif\u001b[39;00m example_obs\u001b[39m.\u001b[39mshape \u001b[39m!=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mobservation_space\u001b[39m.\u001b[39mshape:\n\u001b[1;32m    365\u001b[0m     \u001b[39mraise\u001b[39;00m \u001b[39mValueError\u001b[39;00m(\n\u001b[1;32m    366\u001b[0m         \u001b[39mf\u001b[39m\u001b[39m\"\u001b[39m\u001b[39mThe Training Config observation space: \u001b[39m\u001b[39m{\u001b[39;00m\u001b[39mself\u001b[39m\u001b[39m.\u001b[39mobservation_space\u001b[39m.\u001b[39mshape\u001b[39m}\u001b[39;00m\u001b[39m does not \u001b[39m\u001b[39m\"\u001b[39m\n\u001b[1;32m    367\u001b[0m         \u001b[39mf\u001b[39m\u001b[39m\"\u001b[39m\u001b[39mmatch the Environment observation shape: \u001b[39m\u001b[39m{\u001b[39;00mexample_obs\u001b[39m.\u001b[39mshape\u001b[39m}\u001b[39;00m\u001b[39m\"\u001b[39m\n\u001b[1;32m    368\u001b[0m     )\n\u001b[0;32m--> 369\u001b[0m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mcheck_reward()\n",
      "File \u001b[0;32m~/Documents/Master Wirtschaftsphysik/Masterarbeit Yale-NUS CQT/Quantum_Optimal_Control/quantumenvironment.py:449\u001b[0m, in \u001b[0;36mQuantumEnvironment.check_reward\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    447\u001b[0m sample_action \u001b[39m=\u001b[39m np\u001b[39m.\u001b[39mzeros(\u001b[39mself\u001b[39m\u001b[39m.\u001b[39maction_space\u001b[39m.\u001b[39mshape)\n\u001b[1;32m    448\u001b[0m batch_action \u001b[39m=\u001b[39m np\u001b[39m.\u001b[39mtile(sample_action, (\u001b[39mself\u001b[39m\u001b[39m.\u001b[39mbatch_size, \u001b[39m1\u001b[39m))\n\u001b[0;32m--> 449\u001b[0m batch_rewards \u001b[39m=\u001b[39m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mperform_action(batch_action)\n\u001b[1;32m    450\u001b[0m mean_reward \u001b[39m=\u001b[39m np\u001b[39m.\u001b[39mmean(batch_rewards)\n\u001b[1;32m    451\u001b[0m \u001b[39mif\u001b[39;00m mean_reward \u001b[39m>\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_reward_check_max:\n",
      "File \u001b[0;32m~/Documents/Master Wirtschaftsphysik/Masterarbeit Yale-NUS CQT/Quantum_Optimal_Control/quantumenvironment.py:543\u001b[0m, in \u001b[0;36mQuantumEnvironment.perform_action\u001b[0;34m(self, actions)\u001b[0m\n\u001b[1;32m    541\u001b[0m \u001b[39mexcept\u001b[39;00m \u001b[39mException\u001b[39;00m \u001b[39mas\u001b[39;00m e:\n\u001b[1;32m    542\u001b[0m     \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mclose()\n\u001b[0;32m--> 543\u001b[0m     \u001b[39mraise\u001b[39;00m e\n\u001b[1;32m    544\u001b[0m \u001b[39mif\u001b[39;00m np\u001b[39m.\u001b[39mmean(reward_table) \u001b[39m>\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_max_return:\n\u001b[1;32m    545\u001b[0m     \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_max_return \u001b[39m=\u001b[39m np\u001b[39m.\u001b[39mmean(reward_table)\n",
      "File \u001b[0;32m~/Documents/Master Wirtschaftsphysik/Masterarbeit Yale-NUS CQT/Quantum_Optimal_Control/quantumenvironment.py:537\u001b[0m, in \u001b[0;36mQuantumEnvironment.perform_action\u001b[0;34m(self, actions)\u001b[0m\n\u001b[1;32m    531\u001b[0m     full_circ \u001b[39m=\u001b[39m qc\u001b[39m.\u001b[39mcompose(input_state_circ, inplace\u001b[39m=\u001b[39m\u001b[39mFalse\u001b[39;00m, front\u001b[39m=\u001b[39m\u001b[39mTrue\u001b[39;00m)\n\u001b[1;32m    532\u001b[0m     \u001b[39mprint\u001b[39m(observables)\n\u001b[1;32m    533\u001b[0m     job \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mestimator\u001b[39m.\u001b[39mrun(\n\u001b[1;32m    534\u001b[0m         circuits\u001b[39m=\u001b[39m[full_circ] \u001b[39m*\u001b[39m batch_size,\n\u001b[1;32m    535\u001b[0m         observables\u001b[39m=\u001b[39m[observables] \u001b[39m*\u001b[39m batch_size,\n\u001b[1;32m    536\u001b[0m         parameter_values\u001b[39m=\u001b[39mparams,\n\u001b[0;32m--> 537\u001b[0m         shots\u001b[39m=\u001b[39m\u001b[39mint\u001b[39m(np\u001b[39m.\u001b[39;49mmax(pauli_shots) \u001b[39m*\u001b[39;49m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mn_shots),\n\u001b[1;32m    538\u001b[0m     )\n\u001b[1;32m    540\u001b[0m     reward_table \u001b[39m=\u001b[39m job\u001b[39m.\u001b[39mresult()\u001b[39m.\u001b[39mvalues\n\u001b[1;32m    541\u001b[0m \u001b[39mexcept\u001b[39;00m \u001b[39mException\u001b[39;00m \u001b[39mas\u001b[39;00m e:\n",
      "\u001b[0;31mTypeError\u001b[0m: unsupported operand type(s) for *: 'NoneType' and 'int'"
     ]
    }
   ],
   "source": [
    "# Define the original action space\n",
    "print('Initial lower bounds:', gate_q_env_config.action_space.low)\n",
    "print('Initial upper bounds:', gate_q_env_config.action_space.high)\n",
    "\n",
    "q_env = QuantumEnvironment(gate_q_env_config)\n",
    "\n",
    "# Apply the RescaleAction wrapper\n",
    "q_env = ClipAction(q_env)\n",
    "q_env = RescaleAction(q_env, min_action=-1.0, max_action=1.0)\n",
    "\n",
    "# Confirm the rescale box dimensions\n",
    "print('Rescaled lower bounds:', q_env.action_space.low)\n",
    "print('Rescaled upper bounds:', q_env.action_space.high)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "gate_name = q_env.target['gate'].name\n",
    "gate_name"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(q_env.backend)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from helper_functions import load_agent_from_yaml_file\n",
    "agent_config  = load_agent_from_yaml_file(file_path='/Users/lukasvoss/Documents/Master Wirtschaftsphysik/Masterarbeit Yale-NUS CQT/Quantum_Optimal_Control/template_configurations/agent_config.yaml')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "agent_config"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from ppo import make_train_ppo\n",
    "\n",
    "ppo_agent = make_train_ppo(agent_config, q_env)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "num_updates = 400"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "training_results = ppo_agent(total_updates=num_updates, print_debug=True, num_prints=40)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import pickle\n",
    "import time\n",
    "\n",
    "plt.style.use('ggplot')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "avg_reward = training_results['avg_reward']\n",
    "std_actions = training_results['std_actions']\n",
    "fidelities = training_results['fidelities']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "job_name = f'{gate_name}-gate-calibration-{int(time.time())}-max-fidelity-{round(max(fidelities), 5)}.pickle'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "job_name = f'{gate_name}-gate-calibration-{int(time.time())}-max-fidelity-{max(fidelities):4%}.pickle'\n",
    "\n",
    "with open(job_name, 'wb') as handle:\n",
    "    pickle.dump(training_results, handle, protocol=pickle.HIGHEST_PROTOCOL)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(f'Final Gate Fidelity: {fidelities[-1]:.4%}')\n",
    "print(f'\\nMax Gate Fidelity: {max(fidelities):.4%}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "reward_history = np.array(q_env.reward_history)\n",
    "mean_rewards = np.mean(reward_history, axis=-1)\n",
    "max_mean = int(np.max(mean_rewards) * 1e4) / 1e4\n",
    "\n",
    "fidelity = np.array(q_env.avg_fidelity_history)\n",
    "mean_fidelity = np.mean(fidelity, axis=-1)\n",
    "max_fidelity = int(np.max(mean_rewards) * 1e4) / 1e4\n",
    "\n",
    "plt.plot(mean_rewards, label=f'Mean Batch Rewards, max: {max_mean}')\n",
    "plt.plot(q_env.avg_fidelity_history, label=f'Mean Batch Gate Fidelity, max: {max_fidelity}')\n",
    "plt.xlabel('Updates')\n",
    "plt.title('CX Learning Curve')\n",
    "plt.legend()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "std_actions_componentwise = list(zip(*std_actions))\n",
    "\n",
    "for ind, param in enumerate(std_actions_componentwise):\n",
    "    plt.plot(np.arange(1, num_updates+1), param, label=r'std $\\theta_{}$'.format(ind+1))\n",
    "\n",
    "plt.xlabel('update step')\n",
    "plt.legend()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print('Final action vector:\\n', training_results['action_vector'])"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "cqt_env",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
